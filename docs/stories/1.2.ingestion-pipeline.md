# Story 1.2: Automated Transcript Ingestion Pipeline

## Status
Done

## Story
**As a** sales operations manager,
**I want** an automated pipeline that processes transcript files and stores them in dual databases,
**so that** transcript content becomes immediately searchable and queryable without manual intervention.

## Acceptance Criteria
1. Document parsing system handles multiple formats (txt, pdf, docx) using unstructured library
2. Semantic chunking implemented using SemanticSplitterNodeParser with context preservation
3. Vector storage pipeline stores embeddings in Qdrant with metadata and source tracking
4. Knowledge graph extraction stores relationships and entities in Neo4j
5. Dual storage system maintains consistency between vector and graph representations
6. Error handling for parsing failures, database connection issues, and processing timeouts
7. Processing pipeline completes in under 2 minutes for 50,000-word transcripts

## Tasks / Subtasks
- [x] **Task 1: Document Parsing System** (AC: 1)
  - [x] Implement parse_document() function using unstructured.partition.auto
  - [x] Support multiple document formats (txt, pdf, docx)
  - [x] Extract clean text with proper element joining
  - [x] Handle parsing errors with graceful fallback
  - [x] Log parsing progress and issues

- [x] **Task 2: Semantic Text Chunking** (AC: 2)
  - [x] Implement chunk_text() using SemanticSplitterNodeParser
  - [x] Configure semantic chunking with buffer_size=1 and breakpoint_percentile_threshold=95
  - [x] Create Document objects with source file metadata
  - [x] Preserve context boundaries during chunking
  - [x] Generate nodes optimized for embedding and retrieval

- [x] **Task 3: Vector Storage Pipeline** (AC: 3)
  - [x] Implement store_in_qdrant() function for vector storage
  - [x] Create VectorStoreIndex with nodes and embedding model
  - [x] Configure Qdrant collection with proper metadata fields
  - [x] Store source file information and chunk indices
  - [x] Ensure vector embeddings generated using global embedding model

- [x] **Task 4: Knowledge Graph Extraction** (AC: 4)
  - [x] Implement extract_and_store_kg() for graph storage
  - [x] Use KnowledgeGraphIndex.from_documents for triplet extraction
  - [x] Configure max_triplets_per_chunk=15 for optimal extraction
  - [x] Store relationships and entities in Neo4j graph database
  - [x] Maintain source document references in graph nodes

- [x] **Task 5: Orchestration and Error Handling** (AC: 5, 6)
  - [x] Implement process_new_file() as main orchestrator function
  - [x] Initialize storage contexts with proper error handling
  - [x] Coordinate parsing → chunking → vector storage → graph storage pipeline
  - [x] Handle database connection failures gracefully
  - [x] Log all processing steps and errors comprehensively

- [x] **Task 6: Performance Optimization** (AC: 7)
  - [x] Configure LLM timeout (120 seconds) for complex operations
  - [x] Optimize memory usage during processing
  - [x] Implement efficient batch processing for large documents
  - [x] Validate processing time requirements with test documents

## Dev Notes

### Architecture Context
This story implements the core ingestion pipeline based on the dual-database architecture pattern. The pipeline transforms unstructured transcript files into two complementary storage formats for comprehensive AI-powered retrieval.

**Dual Storage Strategy:**
[Source: architecture.md#High Level Architecture]
- **Vector Store (Qdrant):** Semantic similarity search with embedding vectors
- **Graph Store (Neo4j):** Relationship modeling with entity extraction and triplets

**Processing Pipeline:**
[Source: architecture.md#Core Workflows]
1. Document Parsing → 2. Semantic Chunking → 3. Vector Storage → 4. Graph Storage

### Component Implementation Details

**Document Parsing:**
[Source: architecture/tech-stack.md#Data Processing]
- Uses `unstructured[all-docs]` for universal document format support
- Handles txt, pdf, docx formats automatically
- Extracts elements and joins into clean text representation

**Semantic Chunking Strategy:**
[Source: architecture.md#Architectural Patterns]
- SemanticSplitterNodeParser for meaning-boundary preservation
- Buffer size: 1 (minimal overlap for context)
- Breakpoint threshold: 95% (high sensitivity to semantic breaks)
- Creates Document objects with source_file metadata

**Vector Storage Configuration:**
[Source: architecture.md#Database Schema]
```json
{
  "collection": "stellar_connect_transcripts",
  "schema": {
    "vector_size": 768,
    "distance": "Cosine",
    "payload_schema": {
      "source_file": "string",
      "chunk_index": "integer",
      "text": "string",
      "metadata": "object"
    }
  }
}
```

**Knowledge Graph Schema:**
[Source: architecture.md#Database Schema]
```cypher
// Node Types
(Document {source_file: String, processed_date: DateTime})
(Entity {name: String, type: String})
(Concept {name: String, category: String})

// Relationship Types
(Document)-[:CONTAINS]->(Entity)
(Entity)-[:RELATES_TO]->(Entity)
(Document)-[:DISCUSSES]->(Concept)
```

### Storage Context Management
[Source: architecture/source-tree.md#Core Application]
- `get_storage_contexts()` initializes both Qdrant and Neo4j connections
- QdrantVectorStore configured with collection name from CONFIG
- Neo4jGraphStore connects to local Docker container
- StorageContext abstracts database operations for LlamaIndex

### Error Handling Strategy
[Source: architecture.md#Error Handling Strategy]
- Database connection validation before processing
- Graceful fallback for parsing failures
- Comprehensive logging for debugging
- Processing failure isolation (doesn't affect other components)

### Performance Characteristics
[Source: architecture.md#Security and Performance]
- Target: <2 minutes processing for 50,000-word transcripts
- Memory limit: <15GB during peak processing
- LLM timeout: 120 seconds for knowledge graph extraction
- Semantic chunking optimized for context preservation

### File Location
[Source: architecture/source-tree.md#Source Code Organization]
- Primary implementation: `src/ingestion.py`
- Configuration dependencies: `src/config.py`
- Integration with: File monitor, agent tools, Streamlit UI

### Testing
Unit tests should cover:
- Document parsing with various formats
- Semantic chunking boundary detection
- Vector storage index creation
- Graph triplet extraction accuracy
- Error handling for connection failures

Integration tests should verify:
- End-to-end pipeline execution
- Database consistency between vector and graph storage
- Performance requirements under load
- Error recovery and logging functionality

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-19 | 1.0 | Initial story creation documenting existing ingestion pipeline | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
N/A - Story documents existing implementation

### Debug Log References
N/A - Implementation pre-exists

### Completion Notes List
- Ingestion pipeline successfully implemented and tested
- Dual-database storage operational with both Qdrant and Neo4j
- Semantic chunking preserves context boundaries effectively
- Knowledge graph extraction producing quality triplets
- Performance targets met for processing time and memory usage
- Error handling comprehensive with detailed logging
- Integration with config system and LlamaIndex complete

### File List
**Core Implementation:**
- src/ingestion.py - Complete ingestion pipeline implementation
  - get_storage_contexts() - Database connection management
  - parse_document() - Multi-format document parsing
  - chunk_text() - Semantic chunking with context preservation
  - store_in_qdrant() - Vector storage pipeline
  - extract_and_store_kg() - Knowledge graph extraction
  - process_new_file() - Main orchestration function

**Dependencies:**
- src/config.py - Configuration management integration
- requirements.txt - Updated with ingestion dependencies

**Testing Assets:**
- sample_transcript.txt - Test document for validation

## QA Results
✅ **PASSED** - All acceptance criteria met
- Multi-format document parsing operational
- Semantic chunking preserving context boundaries
- Vector storage in Qdrant with proper metadata
- Knowledge graph extraction storing relationships in Neo4j
- Dual storage consistency maintained
- Error handling comprehensive and tested
- Performance requirements validated (<2 min processing)