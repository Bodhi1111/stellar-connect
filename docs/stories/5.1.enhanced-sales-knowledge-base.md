# Story 5.1: Enhanced Sales Knowledge Base with Multi-Layered Understanding

## Status
Draft

## Story
**As a** CRO and sales strategist,
**I want** to implement structure-aware sales conversation processing with multi-layered understanding,
**so that** the system can extract deeper revenue insights beyond simple text retrieval.

## Acceptance Criteria
1. Structure-aware transcript parsing that identifies sales methodology sections (discovery, needs analysis, solution presentation, objection handling, closing attempts, next steps)
2. Multi-layered sales knowledge base with raw conversations, structured sales elements, semantic embeddings, win/loss patterns, and predictive revenue insights
3. Sales-specific metadata generation including deal size indicators, sales stage classification, objection categorization, and closing probability predictions
4. Enhanced chunking strategy that preserves sales conversation context and methodology relationships
5. Automated summary and keyword generation for each sales conversation layer using Mistral-7B
6. Integration with existing Qdrant and Neo4j infrastructure for sales intelligence
7. Performance optimization maintaining <15GB memory usage during enhanced sales processing

## Tasks / Subtasks
- [ ] **Task 1: Sales Methodology Structure Parser** (AC: 1)
  - [ ] Create SalesStructureParser class for sales methodology section identification
  - [ ] Implement discovery phase detection (question patterns, information gathering)
  - [ ] Add needs analysis section identification (pain points, requirements)
  - [ ] Create solution presentation section parser (product features, benefits)
  - [ ] Implement objection handling detection (concerns, responses, resolution)
  - [ ] Add closing attempts identification (trial closes, commitment requests)
  - [ ] Create next steps section parser (follow-up actions, timelines)
  - [ ] Integrate with existing ingestion pipeline in src/ingestion.py

- [ ] **Task 2: Multi-Layered Knowledge Base Architecture** (AC: 2)
  - [ ] Design enhanced storage schema for multi-layered sales data
  - [ ] Create SalesKnowledgeBase class for layered data management
  - [ ] Implement raw conversation storage with original transcript preservation
  - [ ] Add structured sales elements layer (methodology sections, key moments)
  - [ ] Create semantic embeddings layer with sales-specific context
  - [ ] Implement win/loss patterns storage with outcome correlation
  - [ ] Add predictive revenue insights layer with ML feature extraction
  - [ ] Integrate with existing Qdrant collection architecture

- [ ] **Task 3: Sales-Specific Metadata Generation** (AC: 3)
  - [ ] Create SalesMetadataExtractor class for automated metadata generation
  - [ ] Implement deal size indicator extraction (verbal cues, value mentions)
  - [ ] Add sales stage classification (early, mid, late, closed)
  - [ ] Create objection categorization system (price, timing, authority, need)
  - [ ] Implement closing probability prediction based on conversation signals
  - [ ] Add territory and market segment metadata extraction
  - [ ] Create competitive mention detection and categorization
  - [ ] Integrate metadata with existing Pydantic data models

- [ ] **Task 4: Enhanced Sales-Aware Chunking Strategy** (AC: 4)
  - [ ] Create SalesAwareChunker class extending SemanticSplitterNodeParser
  - [ ] Implement methodology-aware boundary detection
  - [ ] Add conversation context preservation across chunks
  - [ ] Create overlap strategy for maintaining sales flow continuity
  - [ ] Implement relationship preservation between chunks (question→answer pairs)
  - [ ] Add sales moment significance weighting (objections, closes)
  - [ ] Create chunk metadata with sales context information
  - [ ] Update existing chunk_text() function in src/ingestion.py

- [ ] **Task 5: Automated Sales Summary and Keyword Generation** (AC: 5)
  - [ ] Create SalesSummaryGenerator class using Mistral-7B/Llama3 model
  - [ ] Implement conversation layer summarization (methodology sections)
  - [ ] Add sales-specific keyword extraction (products, objections, outcomes)
  - [ ] Create sentiment analysis for each sales methodology section
  - [ ] Implement momentum tracking (engagement level changes)
  - [ ] Add competitive intelligence keyword extraction
  - [ ] Create summary hierarchy (section → conversation → deal progression)
  - [ ] Integrate with global LLM settings from src/config.py

- [ ] **Task 6: Infrastructure Integration and Optimization** (AC: 6, 7)
  - [ ] Extend existing Qdrant collection schema for multi-layered data
  - [ ] Update Neo4j graph schema for sales methodology relationships
  - [ ] Modify get_storage_contexts() in src/ingestion.py for enhanced storage
  - [ ] Implement memory-efficient processing for large sales datasets
  - [ ] Add batch processing capabilities for historical transcript enhancement
  - [ ] Create migration scripts for existing transcript data enhancement
  - [ ] Implement performance monitoring for memory usage tracking
  - [ ] Update process_new_file() function for enhanced processing pipeline

## Dev Notes

### Architecture Context
This story implements the foundation for advanced agentic RAG by transforming the existing simple text retrieval into a sophisticated sales-aware knowledge system that understands sales methodology and conversation structure.

**Enhanced RAG Architecture:**
[Source: architecture.md#Architectural Patterns]
- Evolves from simple semantic chunking to sales methodology-aware processing
- Adds multiple layers of understanding: structure, semantics, patterns, predictions
- Maintains existing dual-database architecture while enhancing capabilities
- Preserves performance characteristics while adding intelligence

**Sales Methodology Framework:**
[Source: docs/prd.md#Epic 5]
Sales conversations follow predictable patterns that can be identified and leveraged:
1. **Discovery:** Question patterns, information gathering behaviors
2. **Needs Analysis:** Pain point identification, requirement articulation
3. **Solution Presentation:** Product positioning, benefit communication
4. **Objection Handling:** Concern raising, response patterns, resolution
5. **Closing Attempts:** Commitment requests, decision points
6. **Next Steps:** Follow-up planning, timeline establishment

### Technical Implementation Details

**Sales Structure Parser Design:**
```python
class SalesStructureParser:
    def __init__(self, llm_model):
        self.llm = llm_model
        self.methodology_patterns = self._load_sales_patterns()

    def parse_methodology_sections(self, transcript: str) -> Dict[str, SalesSection]:
        # Uses LLM to identify sales methodology sections
        # Returns structured sections with boundaries and content
        pass

    def identify_sales_moments(self, section: SalesSection) -> List[SalesMoment]:
        # Identifies key moments within sections (objections, trial closes)
        pass
```

**Multi-Layered Storage Schema:**
[Source: architecture.md#Database Schema]
Enhanced Qdrant collection structure:
```json
{
  "collection": "stellar_connect_enhanced",
  "vectors": {
    "semantic": "768-dim embeddings for content similarity",
    "sales_context": "384-dim embeddings for sales methodology",
    "outcome_pattern": "256-dim embeddings for win/loss prediction"
  },
  "payload": {
    "sales_metadata": {
      "methodology_section": "string",
      "sales_stage": "string",
      "deal_size_indicator": "float",
      "objection_category": "array",
      "closing_probability": "float"
    }
  }
}
```

**Enhanced Neo4j Schema:**
```cypher
// Enhanced nodes for sales methodology
(Conversation {id: String, outcome: String, deal_value: Float})
(SalesSection {type: String, sentiment: Float, duration: Int})
(SalesMoment {type: String, significance: Float, timestamp: Int})
(Objection {category: String, resolved: Boolean, response_quality: Float})

// Enhanced relationships
(Conversation)-[:CONTAINS_SECTION]->(SalesSection)
(SalesSection)-[:HAS_MOMENT]->(SalesMoment)
(SalesMoment)-[:TRIGGERS_OBJECTION]->(Objection)
```

**Sales Metadata Structure:**
[Source: src/data_models.py enhancement]
```python
class EnhancedSalesRecord(SalesRecord):
    sales_stage: Literal["discovery", "needs_analysis", "presentation", "objection_handling", "closing", "next_steps"]
    deal_size_indicator: Optional[float] = None
    objection_categories: List[str] = Field(default_factory=list)
    closing_probability: Optional[float] = None
    methodology_sections: Dict[str, str] = Field(default_factory=dict)
    sales_momentum: Optional[float] = None
    competitive_mentions: List[str] = Field(default_factory=list)
```

### Enhanced Chunking Strategy
[Source: architecture.md#Architectural Patterns - Semantic Chunking]
- **Methodology Awareness:** Chunk boundaries respect sales section boundaries
- **Context Preservation:** Maintain question-answer relationships across chunks
- **Significance Weighting:** Important sales moments get separate chunks
- **Overlap Strategy:** Strategic overlap to maintain conversation flow
- **Metadata Enrichment:** Each chunk tagged with sales context

### Integration with Existing Infrastructure
[Source: architecture/source-tree.md#Core Application]
- **src/ingestion.py:** Enhance process_new_file() with sales-aware processing
- **src/config.py:** Add configuration for enhanced processing modes
- **src/data_models.py:** Extend with enhanced sales metadata models
- **Existing databases:** Backward compatible enhancement of schemas

### Performance Optimization Strategy
[Source: architecture.md#Security and Performance]
- **Memory Management:** Streaming processing for large transcripts
- **Batch Processing:** Efficient enhancement of historical data
- **Selective Enhancement:** Process only new transcripts with full enhancement
- **Caching:** Cache methodology patterns and sales context embeddings
- **Resource Monitoring:** Track memory usage during enhanced processing

### Model Integration
[Source: architecture/tech-stack.md#AI and Machine Learning Stack]
- **Primary LLM:** llama3:8b-instruct for sales analysis and summarization
- **Embedding Models:** nomic-embed-text plus sales-specific embeddings
- **Processing Pipeline:** Enhanced ingestion with sales methodology awareness
- **Timeout Handling:** Extended timeouts for complex sales analysis

### Testing Strategy
Unit tests should cover:
- Sales methodology section identification accuracy
- Metadata extraction precision and recall
- Enhanced chunking boundary preservation
- Memory usage during enhanced processing
- Integration with existing storage systems

Integration tests should verify:
- End-to-end enhanced processing pipeline
- Multi-layered storage consistency
- Performance requirements under enhanced processing
- Backward compatibility with existing data

Performance tests should validate:
- <15GB memory usage during enhancement
- Processing time impact of enhanced analysis
- Storage efficiency of multi-layered approach
- Query performance with enhanced metadata

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-19 | 1.0 | Initial story creation for enhanced sales knowledge base | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
N/A - Story in draft status

### Debug Log References
N/A - Implementation pending

### Completion Notes List
N/A - Implementation pending

### File List
N/A - Implementation pending

## QA Results
N/A - Story in draft status pending implementation